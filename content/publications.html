<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sukrit's Publications</title>
    <link rel="stylesheet" href="style.css">
    <style>
        .justified-text {
            text-align: justify;
        }
    </style>
</head>
<body>
    <header>
        <h1>Publications</h1>
        <p>Go to <a href="../index.html">homepage</a>.</p>
    </header>

    <section id="RL-GBWM">
        <h2>Reinforcement learning for Multiple Goals in Goals-Based Wealth Management</h2>
        <div class="justified-text">
            <p>This research addresses the complexities investors face when managing multiple financial goals, such as purchasing a home, funding education, or planning for retirement. Traditional financial planning methods often struggle to dynamically allocate resources among competing objectives, especially under uncertain market conditions.</p>
            <p>The authors propose a reinforcement learning (RL) framework to optimize portfolio strategies across multiple goals. The RL agent learns to make investment decisions that balance the trade-offs between different objectives, adapting to changing financial landscapes and individual preferences. Key contributions of the paper include:</p>
            <ol>
                <li><b>Dynamic Goal Prioritization:</b> The RL model dynamically adjusts the prioritization of financial goals based on real-time portfolio performance and evolving investor circumstances.</li>
                <li><b>Adaptation to Market Uncertainty:</b> By interacting with simulated market environments, the RL agent learns to navigate uncertainties, enhancing the robustness of investment strategies.</li>
                <li><b>Personalized Investment Strategies:</b> The framework accommodates individual investor preferences, tailoring strategies to align with specific risk tolerances and goal importance.</li>
            </ol>
            <p>The study's findings suggest that RL-based approaches can outperform traditional static allocation methods, offering a more flexible and responsive tool for financial advisors and investors aiming to achieve multiple financial objectives.</p>
            <p>For more details, read the full paper <a href="https://ieeexplore.ieee.org/abstract/document/10771457">here</a>.</p>
        </div>
    </section>

    <section id="UIP">
        <h2>A Unified Innovized Progress Operator for Performance Enhancement in Evolutionary Multi- and Many-Objective Optimization</h2>
        <div class="justified-text">
            <p>This research introduces a machine learning-based Unified Innovized Progress (UIP) operator designed to simultaneously enhance both convergence and diversity in reference vector-based evolutionary multi- and many-objective optimization algorithms (RV-EMOAs). Traditional evolutionary algorithms often face challenges in efficiently balancing convergence towards the Pareto front and maintaining a diverse set of solutions across it. The UIP operator addresses these challenges by:</p>
            <ol>
                <li><b>Convergence Enhancement:</b> It captures efficient search directions by mapping inter-generational solutions along different reference vectors.</li>
                <li><b>Diversity Enhancement:</b> It improves the spread and uniformity of solutions by mapping intra-generational solutions across reference vectors.</li>
            </ol>
            <p>A key advantage of the UIP operator is its generic applicability to various RV-EMOAs without necessitating additional solution evaluations beyond those required by the base algorithms.</p>
            <p>The study's extensive experimental evaluation, comprising 24,056 runs on various multi- and many-objective problems, demonstrated that integrating the UIP operator with different RV-EMOAs resulted in statistically superior performance in approximately 36% of instances and equivalent or better performance in about 92% of cases compared to the respective base algorithms.</p>
            <p>For more details, read the full paper <a href="https://ieeexplore.ieee.org/document/10269130">here</a>.</p>
        </div>
    </section>

    <section id="LHFiD">
        <h2>A Localized High-Fidelity-Dominance-Based Many-Objective Evolutionary Algorithm</h2>
        <div class="justified-text">
            <p>This research addresses challenges in many-objective optimization, particularly the limitations of traditional Pareto-dominance methods when dealing with problems involving four or more objectives. The authors introduce the High-Fidelity-Dominance (HFiD) principle, which simultaneously considers three Human Decision-Making (HDM) elements:</p>
            <ol>
                <li><b>Number of Improved Objectives:</b> Assessing how many objectives a solution improves upon compared to another.</li>
                <li><b>Extent of Improvements:</b> Evaluating the magnitude of these improvements.</li>
                <li><b>Relative Preferences Among Objectives:</b> Incorporating decision-makers' preferences regarding the importance of different objectives.</li>
            </ol>
            <p>Building upon the HFiD principle, the paper proposes the Localized High-Fidelity-Dominance (LHFiD) approach. This method integrates the HFiD principle within a reference vector-based framework, aiming to enhance the search efficiency and solution quality in many-objective evolutionary algorithms.</p>
            <p>The study includes an extensive experimental evaluation, comprising 41,912 experiments, to compare the performance of the LHFiD approach against existing many-objective evolutionary algorithms. The results demonstrate that LHFiD offers competitive advantages in handling complex optimization problems.</p>
            <p>For more details, read the full paper <a href="https://ieeexplore.ieee.org/document/9814856">here</a>.</p>
        </div>
    </section>

    

    <footer>
        <p>Â© 2025 Sukrit Mittal | <a href="mailto:mittalsukrit@gmail.com">Contact</a></p>
    </footer>
</body>
</html>
